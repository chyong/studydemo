1.netty:
（1）IO模型：
    a.阻塞io模型：一个线程处理一个请求，如果数据未到达线程会一直阻塞直到数据到达。比较费服务器资源（占用线程较多），适合连接数目小且固定的架构，但程序简单易理解。
    b.非阻塞io模型：一个线程处理一个请求，不需要等待数据到达，会立马返回数据到达结果，如果未到达会一直轮循（while循环方式）直到数据到达。while循环方式访问内核数据到达状态，不会释放cpu，cpu占用较高。
    c.多路复用io模型：一个线程管理多个socket（连接），该线程会一直轮循（内核进行非用户线程）每个socket是否有读写事件，如果有才会调用实际的io读写操作。适合连接数比较多的情况，响应体较大，会导致后续的事件迟迟得不到处理。
（2）NIO：
    a.同步非阻塞IO模型，三大核心部分-channel（管道）、buffer（缓冲区）、selector（多路复用器），channel提供文件、网络读取数据的渠道，数据读写数据必须经由缓冲区，channel要注册到selector上，一个selector可以注册多个channel，
      selector轮循检测注册在它上面的channel是否有读写事件发生，如果有则进行响应处理。
    b.从JDK1.5开始，nio采用epoll基于事件响应机制进行了优化，epoll（selector的底层实现）是操作系统函数（native方法）
      NioSelectorServer中的几个重要方法：
      Selector.open() //创建selector，底层调用epoll_create方法，创建epoll实例
      socketChannel.register() //将socketChannel注册到selector上，底层先将fd（socketChannel）添加到内部集合中，然后调用epoll_ctl方法进行事件绑定
      selector.select() //阻塞等待需要处理的事件发生，底层调用epoll_wait方法阻塞等待事件发生，然后进行响应处理
    c.epoll函数：int epoll_create(int size);  int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event);  int epoll_wait(int epfd, struct epoll_event *events, int maxevents, int timeout);
（3）Netty线程模型：
    a.netty抽象化出了两组线程池BossGroup和WorkerGroup，BossGroup专门负责接收客户端的连接，WorkerGroup专门负责网络的读写
    b.BossGroup和WorkerGroup类型都是NioEventLoopGroup
    c.NioEventLoopGroup相当于一个事件循环线程组，这个组中含有多个事件循环线程，每一个事件循环线程是NioEventLoop
    d.每个NioEventLoop都有一个selector，用于监听注册在其上的socketChannel的网络通讯，NioEventLoop中维护了一个线程和任务队列，支持异步提交执行任务
    e.每个Boss  NioEventLoop线程内部循环执行的步骤：
      1.处理accept事件，与client建立连接，生成NioSocketChannel
      2.将NioSocketChannel注册到某个worker  NIOEventLoop上的selector
      3.处理任务队列的任务，即runAllTasks
    f.每个worker  NioEventLoop线程循环执行的步骤：
      1.轮询注册到自己selector上的所有NioSocketChannel的read，write事件
      2.处理I/O事件，即read，write 事件，在对应NioSocketChannel处理业务
      3.runAllTasks处理任务队列TaskQueue的任务，一些耗时的业务处理一般可以放入TaskQueue中慢慢处理，这样不影响数据在pipeline中的流动处理
    g.每个worker NIOEventLoop处理NioSocketChannel业务时，会使用 pipeline (管道)，管道中维护了很多ChannelHandler处理器用来处理channel中的数据
      一个Channel包含了一个ChannelPipeline，而ChannelPipeline中又维护了一个由ChannelHandlerContext组成的双向链表，并且每个ChannelHandlerContext中又关联着一个ChannelHandler
    h.ByteBuf：从结构上来说，ByteBuf由一串字节数组构成。数组中每个字节用来存放信息。ByteBuf提供了两个索引，一个用于读取数据，一个用于写入数据。这两个索引通过在字节数组中移动，来定位需要读或者写信息的位置。
              当从ByteBuf读取时，它的readerIndex（读索引）将会根据读取的字节数递增。同样，当写ByteBuf时，它的writerIndex也会根据写入的字节数进行递增。需要注意的是极限的情况是readerIndex刚好读到了writerIndex写入的地方。
              如果readerIndex超过了writerIndex的时候，Netty会抛出IndexOutOf-BoundsException异常。
    i.netty编解码：当netty发送或者接受一个消息的时候，就将会发生一次数据转换。入站消息会被解码：从字节转换为另一种格式（比如java对象）；如果是出站消息，它会被编码成字节。当你通过Netty发送或者接受一个消息的时候，就将会发生一次数据转换。
                 入站消息会被解码：从字节转换为另一种格式（比如java对象）；如果是出站消息，它会被编码成字节。
    j.netty粘包拆包：TCP是一个流协议，就是没有界限的一长串二进制数据。TCP作为传输层协议并不不了解上层业务数据的具体含义，它会根据TCP缓冲区的实际情况进行数据包的划分，所以在业务上认为是一个完整的包，可能会被TCP拆分成多个包进行发送，也有
                  可能把多个小的包封装成一个大的数据包发送，这就是所谓的TCP粘包和拆包问题。面向流的通信是无消息保护边界的。
                  解决方案
                  1）消息定长度，传输的数据大小固定长度，例如每段的长度固定为100字节，如果不够空位补空格
                  2）在数据包尾部添加特殊分隔符，比如下划线，中划线等，这种方法简单易行，但选择分隔符的时候一定要注意每条数据的内部一定不
                  能出现分隔符。
                  3）发送长度：发送每条数据的时候，将数据的长度一并发送，比如可以选择每条数据的前4位是数据的长度，应用层处理时可以根据长度
                  来判断每条数据的开始和结束。
                  netty提供了多个解码器，可以进行分包的操作，如下：
                    LineBasedFrameDecoder（回车换行分包）
                    DelimiterBasedFrameDecoder（特殊分隔符分包）
                    FixedLengthFrameDecoder（固定长度报文来分包）
    k.netty心跳检测机制：所谓心跳，即在TCP长连接中，客户端和服务器之间定期发送的一种特殊的数据包，通知对方自己还在线，以确保TCP连接的有效性。TCP 实际上自带的就有长连接选项，本身是也有心跳包机制，也就是TCP的选项：SO_KEEPALIVE。
                      但是，TCP协议层面的长连接灵活性不够。所以，一般情况下我们都是在应用层协议上实现自定义心跳机制的，也就是在netty层面通过编码实现。通过netty实现心跳机制的话，核心类是IdleStateHandler。